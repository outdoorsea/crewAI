# ğŸ’» Terminal Interface Guide

Complete guide for using Myndy AI pipeline servers directly from the command line with real-time logging and interactive features.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Server Options](#server-options)
- [Interactive Terminal Interface](#interactive-terminal-interface)
- [Single Command Execution](#single-command-execution)
- [Batch Processing](#batch-processing)
- [Real-Time Monitoring](#real-time-monitoring)
- [Advanced Usage](#advanced-usage)
- [Examples](#examples)

---

## ğŸ” Overview

The Myndy AI pipeline provides multiple ways to interact with your AI agents directly from the terminal:

1. **ğŸ–¥ï¸ Server Mode**: Run as a web server with real-time logs
2. **ğŸ’¬ Interactive Mode**: Chat directly in terminal
3. **âš¡ Single Commands**: Execute one-off queries
4. **ğŸ“ Batch Mode**: Process multiple commands from files

---

## ğŸ–¥ï¸ Server Options

### **Option 1: Enhanced Logging Server** â­ **Recommended**

**Purpose**: Run as web server with detailed, colorful logs

**Command**:
```bash
cd /Users/jeremy/crewAI
source venv/bin/activate
python pipeline/server_with_logs.py
```

**Features**:
- ğŸ¨ **Colorful logs** with emojis
- â±ï¸ **Performance metrics** for each request
- ğŸ” **Detailed error tracking**
- ğŸ’¬ **Chat request analysis**
- ğŸ“Š **Model selection insights**

**Output Example**:
```
ğŸš€ Starting Myndy AI Pipeline with Enhanced Logging
============================================================
ğŸ“Š Pipeline Type: Simple
ğŸ–¥ï¸  Real-time logs will appear below...
ğŸŒ Server will be available at: http://localhost:9099
ğŸ”— Add to OpenWebUI: http://localhost:9099
â¹ï¸  Press Ctrl+C to stop
============================================================

ğŸš€ [14:32:15] INFO     __main__              | ğŸš€ Pipeline server starting up...
ğŸ“Š [14:32:15] INFO     __main__              | ğŸ“Š Available models: 6
ğŸ“‹ [14:32:15] INFO     uvicorn               | Uvicorn running on http://0.0.0.0:9099

# When requests come in:
ğŸ“¥ [14:32:30] INFO     __main__              | ğŸ“¥ POST /v1/chat/completions from 127.0.0.1
ğŸ’¬ [14:32:30] INFO     __main__              | ğŸ’¬ Processing: "What's the weather?"
ğŸ¯ [14:32:30] INFO     __main__              | ğŸ¯ Selected: personal_assistant
âš¡ [14:32:30] INFO     __main__              | âš¡ Completed in 0.152s
ğŸ“¤ [14:32:30] INFO     __main__              | ğŸ“¤ âœ… 200 | Response: 245 chars
```

### **Option 2: Simple Server**

**Purpose**: Basic web server with minimal logging

**Command**:
```bash
cd /Users/jeremy/crewAI
source venv/bin/activate
python pipeline/simple_server.py
```

**Features**:
- ğŸ“ **Basic request logging**
- âš¡ **Fast startup**
- ğŸ”§ **Minimal dependencies**

### **Option 3: Uvicorn with Custom Levels**

**Debug Mode** (Most Verbose):
```bash
python -m uvicorn pipeline.server_with_logs:app --host 0.0.0.0 --port 9099 --log-level debug --reload
```

**Production Mode**:
```bash
python -m uvicorn pipeline.server_with_logs:app --host 0.0.0.0 --port 9099 --log-level info
```

---

## ğŸ’¬ Interactive Terminal Interface

### **Purpose**: Chat directly with Myndy AI in terminal

**Command**:
```bash
cd /Users/jeremy/crewAI/pipeline
source ../venv/bin/activate
python terminal_runner.py
```

### **Features**:
- ğŸ¯ **Model selection** (auto or specific agent)
- ğŸ’¬ **Continuous conversation** with history
- ğŸ”„ **Model switching** during chat
- ğŸ—‘ï¸ **History clearing**
- âŒ¨ï¸ **Easy commands**: `quit`, `clear`, `switch`

### **Sample Session**:
```bash
$ python terminal_runner.py

ğŸ§  Myndy AI Terminal Interface
==================================================
Available models:
  1. ğŸ§  Myndy AI v0.1
  2. ğŸ¯ Memory Librarian
  3. ğŸ¯ Research Specialist
  4. ğŸ¯ Personal Assistant
  5. ğŸ¯ Health Analyst
  6. ğŸ¯ Finance Tracker

Select model (1-6, or 'auto' for intelligent routing): auto

ğŸ¯ Using: ğŸ§  Myndy AI v0.1
ğŸ’¬ Start chatting! (Type 'quit' to exit, 'clear' to clear history, 'switch' to change model)
--------------------------------------------------

ğŸ‘¤ You: What's the weather in San Francisco?
ğŸ¤” Thinking...
ğŸ¤– Myndy: ğŸ¤– **Personal Assistant** (Myndy AI)
**Routing:** Selected Personal Assistant based on pattern matching (score: 3)

**Response:** ğŸ—“ï¸ **Personal Assistant**: I would help with weather information for San Francisco.

ğŸ‘¤ You: Do you know John Doe?
ğŸ¤” Thinking...
ğŸ¤– Myndy: ğŸ¤– **Memory Librarian** (Myndy AI)
**Routing:** Selected Memory Librarian based on pattern matching (score: 2)

**Response:** ğŸ“š **Memory Search**: I would search for John Doe in your contacts and knowledge base.

ğŸ‘¤ You: switch
Select model (1-6, or 'auto' for intelligent routing): 2
ğŸ¯ Switched to: ğŸ¯ Memory Librarian

ğŸ‘¤ You: Search for contacts
ğŸ¤” Thinking...
ğŸ¤– Myndy: ğŸ¯ **Memory Librarian** (Direct selection)
**Response:** ğŸ“š **Memory Search**: I would search your contact database for stored information.

ğŸ‘¤ You: clear
ğŸ—‘ï¸  Conversation history cleared!

ğŸ‘¤ You: quit
ğŸ‘‹ Goodbye!
```

### **Interactive Commands**:

| Command | Action |
|---------|---------|
| `quit`, `exit`, `q` | Exit the program |
| `clear` | Clear conversation history |
| `switch` | Change to different model |
| Normal text | Send message to AI |

---

## âš¡ Single Command Execution

### **Purpose**: Execute one-off queries quickly

**Basic Usage**:
```bash
cd /Users/jeremy/crewAI/pipeline
source ../venv/bin/activate
python single_command.py "Your message here"
```

### **With Specific Model**:
```bash
python single_command.py "Your message here" model_name
```

### **Examples**:

**Auto-routing**:
```bash
python single_command.py "What's the weather like?"
```
Output:
```
ğŸ§  Myndy AI Processing: What's the weather like?
ğŸ¯ Using model: auto
--------------------------------------------------
ğŸ¤– Response: ğŸ¤– **Personal Assistant** (Myndy AI)
**Routing:** Selected Personal Assistant based on pattern matching (score: 3)
**Response:** I would help with weather information...
```

**Specific Agent**:
```bash
python single_command.py "Search for John Doe" memory_librarian
```
Output:
```
ğŸ§  Myndy AI Processing: Search for John Doe
ğŸ¯ Using model: memory_librarian
--------------------------------------------------
ğŸ¤– Response: ğŸ¯ **Memory Librarian** (Direct selection)
**Response:** I would search for John Doe in your contacts...
```

**Multiple Quick Commands**:
```bash
# Weather query
python single_command.py "Current weather in NYC"

# Contact search
python single_command.py "Find Sarah Chen" memory_librarian

# Financial query
python single_command.py "Track my expenses" finance_tracker

# Health query
python single_command.py "Analyze my sleep patterns" health_analyst

# Research query
python single_command.py "Research AI trends" research_specialist
```

---

## ğŸ“ Batch Processing

### **Purpose**: Process multiple commands from files or interactively

**Interactive Mode**:
```bash
cd /Users/jeremy/crewAI/pipeline
source ../venv/bin/activate
python batch_processor.py
```

**From File**:
```bash
python batch_processor.py -i commands.txt -o results.json
```

### **Creating Command Files**:

**Example commands.txt**:
```
# Weather queries
What's the weather in San Francisco?
Check weather in New York
Temperature in London today

# Contact searches  
Do you know John Doe?
Find Sarah Chen's contact info
Search for contacts at Google

# Research requests
Research latest AI developments
Analyze current tech trends
Summarize blockchain news

# Financial queries
Track my spending this month
Budget analysis for Q4
Recent expense patterns
```

### **Running Batch Processing**:

**Step 1**: Create command file
```bash
cat > my_commands.txt << 'EOF'
What time is it?
Do you know Jeremy?
Research Python frameworks
Track my expenses
Check my health data
EOF
```

**Step 2**: Process commands
```bash
python batch_processor.py -i my_commands.txt -o my_results.json
```

**Sample Output**:
```
[1] Processing: What time is it?
âœ… Response: ğŸ¤– **Personal Assistant** (Myndy AI)...

[2] Processing: Do you know Jeremy?
âœ… Response: ğŸ¤– **Memory Librarian** (Myndy AI)...

[3] Processing: Research Python frameworks
âœ… Response: ğŸ¤– **Research Specialist** (Myndy AI)...

[4] Processing: Track my expenses
âœ… Response: ğŸ¤– **Finance Tracker** (Myndy AI)...

[5] Processing: Check my health data
âœ… Response: ğŸ¤– **Health Analyst** (Myndy AI)...

ğŸ’¾ Results saved to: my_results.json
ğŸ‰ Processed 5 commands
```

**Step 3**: View results
```bash
cat my_results.json | jq '.[] | {command: .command, success: .success}'
```

---

## ğŸ” Real-Time Monitoring

### **Monitoring Server Performance**

**Terminal 1** (Start server with logs):
```bash
cd /Users/jeremy/crewAI
source venv/bin/activate
python pipeline/server_with_logs.py
```

**Terminal 2** (Send test requests):
```bash
# Test basic functionality
curl -s http://localhost:9099/v1/models | jq '.data[].name'

# Test different agents
curl -X POST http://localhost:9099/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"auto","messages":[{"role":"user","content":"Weather test"}]}' \
  | jq '.choices[0].message.content'

# Monitor specific agent
curl -X POST http://localhost:9099/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"memory_librarian","messages":[{"role":"user","content":"Contact search test"}]}'
```

**Terminal 1 will show**:
```
ğŸ“¥ [15:30:15] INFO     __main__              | ğŸ“¥ GET /v1/models from 127.0.0.1
ğŸ“‹ [15:30:15] INFO     __main__              | ğŸ“‹ Models endpoint accessed
ğŸ“¤ [15:30:15] INFO     __main__              | ğŸ“¤ âœ… 200 | 0.003s | /v1/models

ğŸ“¥ [15:30:20] INFO     __main__              | ğŸ“¥ POST /v1/chat/completions from 127.0.0.1
ğŸ’¬ [15:30:20] INFO     __main__              | ğŸ’¬ Processing chat request:
ğŸ“‹ [15:30:20] INFO     __main__              |    ğŸ¯ Model: auto
ğŸ“‹ [15:30:20] INFO     __main__              |    ğŸ“ Message: Weather test
âš¡ [15:30:20] INFO     __main__              | âš¡ Pipeline processing completed in 0.095s
ğŸ“¤ [15:30:20] INFO     __main__              | ğŸ“¤ âœ… 200 | 0.102s | /v1/chat/completions
```

### **Performance Testing**

**Load Testing Script**:
```bash
cat > test_load.sh << 'EOF'
#!/bin/bash
echo "ğŸš€ Load testing Myndy AI pipeline..."
echo "Starting $(date)"

for i in {1..20}; do
  echo -n "Request $i: "
  start_time=$(date +%s.%N)
  
  response=$(curl -s -X POST http://localhost:9099/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d "{\"model\":\"auto\",\"messages\":[{\"role\":\"user\",\"content\":\"Load test $i\"}]}")
  
  end_time=$(date +%s.%N)
  duration=$(echo "$end_time - $start_time" | bc)
  
  if echo "$response" | jq -e '.choices[0].message.content' > /dev/null 2>&1; then
    echo "âœ… ${duration}s"
  else
    echo "âŒ Failed"
  fi
  
  sleep 0.1
done

echo "Completed $(date)"
EOF

chmod +x test_load.sh
./test_load.sh
```

---

## ğŸ”§ Advanced Usage

### **Environment Variables**

Set environment variables for customization:
```bash
export MYNDY_PATH="/Users/jeremy/myndy"
export PIPELINE_PORT="9099"
export LOG_LEVEL="DEBUG"
export PIPELINE_MODE="development"

python pipeline/server_with_logs.py
```

### **Custom Configuration**

Create custom pipeline config:
```bash
cat > pipeline_config.json << 'EOF'
{
  "server": {
    "host": "0.0.0.0",
    "port": 9099,
    "log_level": "INFO"
  },
  "pipeline": {
    "type": "enhanced",
    "myndy_path": "/Users/jeremy/myndy",
    "enable_caching": true,
    "enable_metrics": true
  },
  "agents": {
    "default_model": "auto",
    "timeout": 30,
    "max_retries": 3
  }
}
EOF

# Use config (if pipeline supports it)
python pipeline/server_with_logs.py --config pipeline_config.json
```

### **Logging to Files**

**Save all logs to file**:
```bash
python pipeline/server_with_logs.py 2>&1 | tee pipeline_$(date +%Y%m%d_%H%M%S).log
```

**Filter specific log types**:
```bash
# Only errors
python pipeline/server_with_logs.py 2>&1 | grep "âŒ\|ğŸš¨" | tee errors.log

# Only performance metrics
python pipeline/server_with_logs.py 2>&1 | grep "âš¡\|â±ï¸" | tee performance.log

# Only chat interactions
python pipeline/server_with_logs.py 2>&1 | grep "ğŸ’¬" | tee conversations.log
```

### **Health Checking**

**Create health check script**:
```bash
cat > health_check.sh << 'EOF'
#!/bin/bash
echo "ğŸ¥ Myndy AI Pipeline Health Check"
echo "================================"

# Check if server is running
if curl -s http://localhost:9099/ > /dev/null; then
  echo "âœ… Server: Running"
  
  # Check models endpoint
  models=$(curl -s http://localhost:9099/v1/models | jq '.data | length')
  echo "âœ… Models: $models available"
  
  # Test chat endpoint
  response=$(curl -s -X POST http://localhost:9099/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{"model":"auto","messages":[{"role":"user","content":"health check"}]}')
  
  if echo "$response" | jq -e '.choices[0].message.content' > /dev/null 2>&1; then
    echo "âœ… Chat: Working"
  else
    echo "âŒ Chat: Failed"
  fi
  
  # Check response time
  start_time=$(date +%s.%N)
  curl -s http://localhost:9099/ > /dev/null
  end_time=$(date +%s.%N)
  response_time=$(echo "$end_time - $start_time" | bc)
  echo "â±ï¸  Response Time: ${response_time}s"
  
else
  echo "âŒ Server: Not running"
  echo "ğŸ’¡ Start with: python pipeline/server_with_logs.py"
fi
EOF

chmod +x health_check.sh
./health_check.sh
```

---

## ğŸ“‹ Examples

### **Example 1: Development Workflow**

**Start development server**:
```bash
cd /Users/jeremy/crewAI
source venv/bin/activate
python pipeline/server_with_logs.py --log-level debug
```

**Test in another terminal**:
```bash
# Quick functionality test
python pipeline/single_command.py "Hello Myndy"

# Interactive testing
python pipeline/terminal_runner.py

# Batch testing
echo -e "Test 1\nTest 2\nTest 3" | python pipeline/batch_processor.py
```

### **Example 2: Production Monitoring**

**Start production server**:
```bash
python pipeline/server_with_logs.py --log-level info 2>&1 | tee production.log
```

**Monitor in real-time**:
```bash
# Watch error logs
tail -f production.log | grep "âŒ\|ğŸš¨"

# Monitor performance
tail -f production.log | grep "âš¡" | while read line; do
  echo "$line" | grep -o "[0-9]\+\.[0-9]\+s"
done
```

### **Example 3: Testing All Agents**

**Create comprehensive test**:
```bash
cat > test_all_agents.txt << 'EOF'
# Test auto-routing
What's the weather in San Francisco?
Do you know John Doe?
Research AI trends
Track my expenses
Check my health data

# Test specific agents
Personal assistant: What time is it?
Memory librarian: Search for contacts
Research specialist: Analyze this topic
Health analyst: Review my fitness
Finance tracker: Show spending patterns
EOF

python pipeline/batch_processor.py -i test_all_agents.txt -o agent_test_results.json
```

### **Example 4: Performance Benchmarking**

**Benchmark script**:
```bash
cat > benchmark.sh << 'EOF'
#!/bin/bash
echo "ğŸš€ Myndy AI Performance Benchmark"
echo "================================="

models=("auto" "memory_librarian" "personal_assistant" "research_specialist" "health_analyst" "finance_tracker")
requests_per_model=5

for model in "${models[@]}"; do
  echo "Testing model: $model"
  total_time=0
  
  for i in $(seq 1 $requests_per_model); do
    start_time=$(date +%s.%N)
    
    curl -s -X POST http://localhost:9099/v1/chat/completions \
      -H "Content-Type: application/json" \
      -d "{\"model\":\"$model\",\"messages\":[{\"role\":\"user\",\"content\":\"Benchmark test $i\"}]}" > /dev/null
    
    end_time=$(date +%s.%N)
    duration=$(echo "$end_time - $start_time" | bc)
    total_time=$(echo "$total_time + $duration" | bc)
    
    echo "  Request $i: ${duration}s"
  done
  
  avg_time=$(echo "scale=3; $total_time / $requests_per_model" | bc)
  echo "  Average: ${avg_time}s"
  echo
done
EOF

chmod +x benchmark.sh
./benchmark.sh
```

---

## ğŸ¯ Quick Reference

### **Starting Servers**
```bash
# Enhanced logging (recommended)
python pipeline/server_with_logs.py

# Simple server  
python pipeline/simple_server.py

# Debug mode
python pipeline/server_with_logs.py --log-level debug

# Different port
python pipeline/server_with_logs.py --port 9100
```

### **Terminal Interfaces**
```bash
# Interactive chat
python pipeline/terminal_runner.py

# Single command
python pipeline/single_command.py "message"

# Batch processing
python pipeline/batch_processor.py -i input.txt -o output.json
```

### **Testing Commands**
```bash
# Health check
curl http://localhost:9099/

# List models
curl http://localhost:9099/v1/models

# Send chat message
curl -X POST http://localhost:9099/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"auto","messages":[{"role":"user","content":"test"}]}'
```

### **Monitoring**
```bash
# Save logs to file
python pipeline/server_with_logs.py 2>&1 | tee logs.txt

# Filter errors only
python pipeline/server_with_logs.py 2>&1 | grep "âŒ"

# Check if running
pgrep -f "server_with_logs.py"
```

---

**ğŸ‰ You're now equipped with comprehensive terminal interface capabilities for Myndy AI!**

Choose the interface that best fits your workflow - from real-time server monitoring to quick command execution to interactive conversations.